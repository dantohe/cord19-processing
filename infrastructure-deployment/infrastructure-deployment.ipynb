{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Resources deployment using Infrastructure-as-Code (on a Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /Users/dantohe/Library/Python/3.8/lib/python/site-packages (22.0.3)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "# !python3 -m pip install boto3\n",
    "# !python3 -m pip install requests\n",
    "# !python3 -m pip install tqdm\n",
    "# !python3 -m pip install pandas\n",
    "# !python3 -m pip install s3fs\n",
    "# !python3 -m pip install ipywidgets\n",
    "# !python3 -m pip install -q -U paramiko\n",
    "# !python3 -m pip install -q -U scp\n",
    "# !pip install -q -U ipython-sql\n",
    "# !pip install -q -U psycopg2-binary\n",
    "# !python3 -m pip install -q -U Pygments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import boto3\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import paramiko\n",
    "import scp\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core\n",
    "my_bucket_name = 'dantohe-my-experimental-iac-02'\n",
    "# the covid-19 data lake is located in us-east-2\n",
    "my_region = 'us-east-2'\n",
    "stem = 'my-experimental'\n",
    "\n",
    "#ec2\n",
    "my_InstanceProfileName = f'{stem}-InstanceProfileName-iac-01'\n",
    "ec2_pem_name      = f'{stem}-kp-june-2021-01'\n",
    "my_role_name = f'{stem}-ec2-role-01'\n",
    "my_security_group_name = f'{stem}-Airflow-security-group-01'\n",
    "# https://aws.amazon.com/ec2/spot/pricing/\n",
    "# t3.2xlarge\n",
    "instance_size = 't3.xlarge'\n",
    "instance_max_price = '0.12'\n",
    "\n",
    "#redshift\n",
    "my_redshift_role_name = f'{stem}-Redshift-role-01'\n",
    "redshift_port = 5439\n",
    "redshift_user = 'redshift'\n",
    "# Only printable ASCII characters except for '/', '@', '\"', ' ', '\\', ''' may be used.\n",
    "redshift_MasterUserPassword = 'kljhdfsKLJDD12345'\n",
    "redshift_db=f'{stem}-capstone-db'\n",
    "redshift_ClusterIdentifier=f'{stem}-redshift-cluster'\n",
    "#https://aws.amazon.com/redshift/pricing/\n",
    "redshift_NodeType='dc2.large'\n",
    "# https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\n",
    "redshift_NumberOfNodes=1\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "redshift_ClusterType='single-node'\n",
    "\n",
    "redshift_destination_external_schema_name = 'spectrum_schema'\n",
    "redshift_destination_glue_database_name = 'covid-19'\n",
    "redshift_destination_table_name = 'alleninstitute_metadata'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = Config(\n",
    "    region_name = my_region,\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "# client = boto3.client('kinesis', config=my_config)\n",
    "ec2 = boto3.client('ec2', config=my_config)\n",
    "iam = boto3.client('iam', config=my_config)\n",
    "redshift = boto3.client('redshift', config=my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an EC2 key-pair    \n",
    "If the key already exists then don't do anything.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-experimental-kp-june-2021-01 has been created sucessfully and the pem is available at\n",
      "./my-experimental-kp-june-2021-01.pem\n"
     ]
    }
   ],
   "source": [
    "key_exists = False\n",
    "response = ec2.describe_key_pairs()['KeyPairs']\n",
    "for key in response:\n",
    "    if key['KeyName'] == ec2_pem_name:\n",
    "        key_exists = True\n",
    "    found_instance = ec2.describe_instances(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'key-name',\n",
    "                'Values': [key['KeyName']]\n",
    "            }\n",
    "        ]\n",
    "    )['Reservations']\n",
    "\n",
    "if key_exists:\n",
    "    print('key already exists')\n",
    "else:\n",
    "    ec2_pem_path = f'./{ec2_pem_name}.pem'\n",
    "    if os.path.isfile(ec2_pem_path):\n",
    "        os.remove(ec2_pem_path)\n",
    "    ec2_keypair = ec2.create_key_pair(KeyName=ec2_pem_name)\n",
    "    with open(ec2_pem_path, 'w+') as ec2_pem_file:\n",
    "        ec2_pem_file.write(str(ec2_keypair['KeyMaterial']))\n",
    "    !chmod 400 {ec2_pem_path}\n",
    "    print(f'{ec2_pem_name} has been created sucessfully and the pem is available at\\n{ec2_pem_path}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Resources    \n",
    "IAM - refrences: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#role   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a role and attach the s3 access policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A utility that checks if a given role already exists    \n",
    "If already in place returns the role object, otherwise returns a None.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_role_already_exist(role_name):\n",
    "    roles = iam.list_roles()\n",
    "    role_list = roles['Roles']\n",
    "    requested_role= None\n",
    "\n",
    "    for role in role_list:\n",
    "        if role['RoleName'] == role_name:\n",
    "            requested_role = role\n",
    "            return requested_role\n",
    "    return requested_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create role for ec2 and attach s3 access policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-experimental-ec2-role-01 has been createed - the S3 policies have been attached\n"
     ]
    }
   ],
   "source": [
    "roles = iam.list_roles()\n",
    "role_list = roles['Roles']\n",
    "ec2_role= None\n",
    "\n",
    "for key in role_list:\n",
    "    if key['RoleName'] == my_role_name:\n",
    "        ec2_role = key\n",
    "\n",
    "if ec2_role is not None:\n",
    "    print(f'Role {my_role_name} already exists')\n",
    "else:\n",
    "    ec2_role = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=my_role_name,\n",
    "        Description='',\n",
    "        MaxSessionDuration=3600,\n",
    "        AssumeRolePolicyDocument=\"\"\"{\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": { \"Service\": \"ec2.amazonaws.com\"},\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "      ]\n",
    "    }\"\"\".replace('<dw_bucket>', my_bucket_name))['Role']\n",
    "    ### Also atach the S3 policy to the role\n",
    "    for ec2_policy in  [\n",
    "        'arn:aws:iam::aws:policy/AmazonS3FullAccess']:\n",
    "        assert iam.attach_role_policy(\n",
    "            RoleName=ec2_role['RoleName'],\n",
    "            PolicyArn=ec2_policy)['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    \n",
    "    print(f'{my_role_name} has been createed - the S3 policies have been attached')\n",
    "#     ec2_role['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates the instance profile AND adds the role to instance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-experimental-InstanceProfileName-iac-01 has been created\n"
     ]
    }
   ],
   "source": [
    "instance_profiles = iam.list_instance_profiles()\n",
    "instance_profiles_list = instance_profiles['InstanceProfiles']\n",
    "ec2_instance_profile = None\n",
    "\n",
    "# existing_instance_profile_names =[]\n",
    "\n",
    "for key in instance_profiles_list:\n",
    "    if key['InstanceProfileName'] == my_InstanceProfileName:\n",
    "        ec2_instance_profile =key\n",
    "#     existing_instance_profile_names.append(key['InstanceProfileName'])\n",
    "\n",
    "# if my_InstanceProfileName in existing_instance_profile_names:\n",
    "#     print(f'{my_InstanceProfileName} already exists')\n",
    "if ec2_instance_profile is not None:\n",
    "    print(f'{my_InstanceProfileName} already exists')\n",
    "else:\n",
    "    #creates the instaance profile\n",
    "    ec2_instance_profile = iam.create_instance_profile(InstanceProfileName=my_InstanceProfileName)['InstanceProfile']\n",
    "    iam.get_waiter('instance_profile_exists').wait(InstanceProfileName=my_InstanceProfileName)\n",
    "\n",
    "    #adds the role to the instance profile\n",
    "    assert iam.add_role_to_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'], RoleName=ec2_role['RoleName'])['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    print(f'{my_InstanceProfileName} has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Createting a security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The security group my-experimental-Airflow-security-group-01 has been created\n",
      "SG ID: sg-0dcfeb9234906d70f\n"
     ]
    }
   ],
   "source": [
    "security_groups = ec2.describe_security_groups()\n",
    "existing_security_groups = security_groups['SecurityGroups']\n",
    "\n",
    "ec2_sg = None\n",
    "\n",
    "for key in existing_security_groups:\n",
    "    if key['GroupName'] == my_security_group_name:\n",
    "      ec2_sg=key  \n",
    "    \n",
    "if ec2_sg is not None:\n",
    "    print(f'The security group {my_security_group_name} already exists')\n",
    "else:\n",
    "    ec2_sg = ec2.create_security_group(\n",
    "        Description='Allows 22 trafic',\n",
    "        GroupName=my_security_group_name)\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=22, ToPort=22, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=8080, ToPort=8080, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=5555, ToPort=5555, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=3306, ToPort=3306, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    print(f'The security group {my_security_group_name} has been created')\n",
    "    print(f\"SG ID: {ec2_sg['GroupId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting spot instance(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in Region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "# !aws configure set region 'us-east-2'\n",
    "my_session = boto3.session.Session()\n",
    "my_region = my_session.region_name\n",
    "ags_west = boto3.client('autoscaling', region_name=my_region)\n",
    "print(f\"We are in Region: {my_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spot instance request: sir-zgaya5xq\n"
     ]
    }
   ],
   "source": [
    "#time.sleep(300) #wait instance profile...\n",
    "#Amazon Linux AMI - it has some issues and complications with installing mysql and airflow\n",
    "# ec2_ami_id = 'ami-0aeeebd8d2ab47354'\n",
    "#defaulting to ubuntu\n",
    "# us-east-1 'ami-09e67e426f25ce0d7'\n",
    "# us-west-2 'ami-03d5c68bab01f3496'\n",
    "# us-east-2 'ami-00399ec92321828f5'\n",
    "ec2_ami_id = 'ami-00399ec92321828f5'\n",
    "ec2_spot = ec2.request_spot_instances(\n",
    "    AvailabilityZoneGroup=my_region,\n",
    "    InstanceCount=1,\n",
    "    LaunchSpecification={\n",
    "        'SecurityGroupIds': [ec2_sg['GroupId']],\n",
    "        'EbsOptimized': False,\n",
    "        'KeyName': ec2_pem_name,\n",
    "        'ImageId': ec2_ami_id,\n",
    "        'InstanceType': instance_size,\n",
    "        'IamInstanceProfile': {\n",
    "            'Arn': ec2_instance_profile['Arn']\n",
    "        },\n",
    "        \"BlockDeviceMappings\": [\n",
    "            {\n",
    "                \"DeviceName\": \"/dev/sda1\",\n",
    "                \"Ebs\": {\n",
    "                        \"DeleteOnTermination\": True,\n",
    "                        \"VolumeSize\": 30,\n",
    "                        \"Encrypted\": False,\n",
    "                        \"VolumeType\": \"gp2\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    SpotPrice=instance_max_price,\n",
    "    Type='one-time',\n",
    "    InstanceInterruptionBehavior='terminate'\n",
    ")\n",
    "ec2_spot_id = ec2_spot['SpotInstanceRequests'][0]['SpotInstanceRequestId']\n",
    "ec2.get_waiter('spot_instance_request_fulfilled').wait(SpotInstanceRequestIds=[ec2_spot_id])\n",
    "print(f'Spot instance request: {ec2_spot_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gets the instance ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InstanceIds: i-0941cc99318e0a3a0\n"
     ]
    }
   ],
   "source": [
    "ec2_vm_id = ec2.describe_spot_instance_requests(SpotInstanceRequestIds=[ ec2_spot_id ]) \\\n",
    "    ['SpotInstanceRequests'] \\\n",
    "    [0] \\\n",
    "    ['InstanceId']\n",
    "ec2.get_waiter('instance_status_ok').wait(InstanceIds=[ ec2_vm_id ])\n",
    "print(f'InstanceIds: {ec2_vm_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocating a public IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PublicIp: 3.134.147.166\n",
      "AllocationId: eipalloc-067f191af1aad4a5d\n"
     ]
    }
   ],
   "source": [
    "ec2_ip = ec2.allocate_address(Domain='vpc')\n",
    "print(f\"PublicIp: {ec2_ip['PublicIp']}\\nAllocationId: {ec2_ip['AllocationId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associates the IP address with the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP AssociationId: eipassoc-04f49dd31e9ad5bef\n"
     ]
    }
   ],
   "source": [
    "ec2_vm_ip = ec2.associate_address(\n",
    "     InstanceId = ec2_vm_id,\n",
    "     AllocationId = ec2_ip[\"AllocationId\"])\n",
    "print(f\"IP AssociationId: {ec2_vm_ip['AssociationId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSH utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ssh(ip, pem_path):\n",
    "    print(f\"ssh -i {pem_path} ubuntu@{ip}\")\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(hostname=ip, username='ubuntu', pkey=paramiko.RSAKey.from_private_key_file(pem_path))\n",
    "    return ssh\n",
    "\n",
    "def run_via_ssh(\n",
    "        ip,\n",
    "        pem_path,\n",
    "        commands,\n",
    "        display_output=False):\n",
    "    \n",
    "    ssh = get_ssh(ip, pem_path)\n",
    "    try:\n",
    "        for command in tqdm(commands):\n",
    "            stdin, stdout, stderr = ssh.exec_command(command)\n",
    "            exit_status = stdout.channel.recv_exit_status()\n",
    "            if exit_status == 0:\n",
    "                print(('command executed successfuly:::', command))\n",
    "                if display_output:\n",
    "                    output_buffer = stdout.read().decode('utf-8')\n",
    "                    if output_buffer:\n",
    "                        print(f\">>> {output_buffer}\")\n",
    "            else:\n",
    "                error_buffer = stderr.read().decode('utf-8')\n",
    "                print(('!!!failed', command))\n",
    "                print(f\"!!! {error_buffer}\")\n",
    "    finally:\n",
    "        ssh.close()\n",
    "\n",
    "# \n",
    "def print_python_code(code):\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import PythonLexer\n",
    "    from pygments.formatters import HtmlFormatter\n",
    "    import IPython\n",
    "    formatter = HtmlFormatter()\n",
    "    return IPython.display.HTML('<style type=\"text/css\">{}</style>{}'.format(\n",
    "        formatter.get_style_defs('.highlight'),\n",
    "        highlight(code, PythonLexer(), formatter)))\n",
    "   \n",
    "    \n",
    "def upload_dag_file(ip, pem_path, file_name, family_dir='airflow', display_file=True):\n",
    "    code = None\n",
    "    file_path = f'{file_name}'\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    if display_file:\n",
    "        with open(file_path) as f:\n",
    "            code = f.read()\n",
    "        if not code:\n",
    "            return None\n",
    "    \n",
    "    ssh = get_ssh(ip, pem_path)\n",
    "    try:\n",
    "        remote_file_dir = f'~/{family_dir}/{file_dir}'\n",
    "        print(f\"scp -i my-experimental-kp-june-2021-01.pem '{file_path}' 'ubuntu@{ip}:{remote_file_dir}'\")\n",
    "        scp_client = scp.SCPClient(ssh.get_transport())\n",
    "        scp_client.put(files=[file_path], remote_path=remote_file_dir, preserve_times=True)\n",
    "    finally:\n",
    "        ssh.close()\n",
    "    \n",
    "    if display_file:\n",
    "        return print_python_code(code)\n",
    "    else:\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instaling Airflow and libraries\n",
    "This can be achieved using a requirements file but for better visibility it will be kept in this format for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('command executed successfuly:::', 'sudo apt-get -y update')\n",
      "('command executed successfuly:::', 'sudo apt-get install -y python3 python3-pip python3-setuptools')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U pip')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U apache-airflow')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U boto3==1.15.0')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U jsonpath_ng==1.5.3')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U pandas==1.4.0')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U redshift_connector==2.0.888')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U sqlalchemy_redshift==0.8.6')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U watchtower==2.0.1')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U apache-airflow-providers-amazon')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U tensorflow')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U pandas')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U scikit-learn')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U numpy')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U psycopg2-binary')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U requests')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U boto3')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U matplotlib')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U reportlab')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U flower')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U proj')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U apache-airflow[postgres]')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U apache-airflow[amazon]')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U tqdm')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U langdetect')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U seaborn')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U gensim')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U nltk')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz')\n",
      "('command executed successfuly:::', 'sudo pip3 install -U redis')\n"
     ]
    }
   ],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'sudo apt-get -y update',\n",
    "#         'sudo apt-get install -y libmysqlclient-dev mysql-server',\n",
    "#         f\"sudo mysql -e \\\"SET GLOBAL explicit_defaults_for_timestamp = 1;\\\"\",\n",
    "#         f\"sudo mysql -e \\\"DROP DATABASE IF EXISTS airflow;\\\"\",       \n",
    "#         f\"sudo mysql -e \\\"CREATE DATABASE airflow CHARACTER SET UTF8mb3 COLLATE utf8_general_ci;\\\"\",\n",
    "#         f\"sudo mysql -e \\\"CREATE USER 'airflow'@'localhost' IDENTIFIED BY 'airflow';\\\"\",\n",
    "#         f\"sudo mysql -e \\\"GRANT ALL PRIVILEGES ON airflow.* TO 'airflow'@'localhost';\\\"\",\n",
    "#         f\"sudo apt install -y redis-server\",\n",
    "        'sudo apt-get install -y python3 python3-pip python3-setuptools',\n",
    "        'sudo pip3 install -U pip',\n",
    "        'sudo pip3 install -U apache-airflow',\n",
    "#         'sudo pip3 install -U apache-airflow[mysql]',\n",
    "#         'sudo pip3 install -U apache-airflow[celery]',\n",
    "        'sudo pip3 install -U boto3==1.15.0',\n",
    "        'sudo pip3 install -U jsonpath_ng==1.5.3',\n",
    "        'sudo pip3 install -U pandas==1.4.0',\n",
    "        'sudo pip3 install -U redshift_connector==2.0.888',\n",
    "        'sudo pip3 install -U sqlalchemy_redshift==0.8.6',\n",
    "        'sudo pip3 install -U watchtower==2.0.1',\n",
    "        'sudo pip3 install -U apache-airflow-providers-amazon',\n",
    "        'sudo pip3 install -U tensorflow',\n",
    "        'sudo pip3 install -U pandas',\n",
    "        'sudo pip3 install -U scikit-learn',\n",
    "        'sudo pip3 install -U numpy',\n",
    "        'sudo pip3 install -U psycopg2-binary',\n",
    "        'sudo pip3 install -U requests',\n",
    "        'sudo pip3 install -U boto3',\n",
    "        'sudo pip3 install -U matplotlib',\n",
    "        'sudo pip3 install -U reportlab',\n",
    "        'sudo pip3 install -U flower',\n",
    "        'sudo pip3 install -U proj',\n",
    "        'sudo pip3 install -U apache-airflow[postgres]',\n",
    "        'sudo pip3 install -U apache-airflow[amazon]',\n",
    "        'sudo pip3 install -U tqdm',\n",
    "        'sudo pip3 install -U langdetect',\n",
    "        'sudo pip3 install -U seaborn',\n",
    "        'sudo pip3 install -U gensim',\n",
    "        'sudo pip3 install -U nltk',\n",
    "        'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz',\n",
    "        'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz',\n",
    "        'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz',\n",
    "        'sudo pip3 install -U redis'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('command executed successfuly:::', 'sudo pip3 install markupsafe==2.0.1')\n",
      "('command executed successfuly:::', 'airflow db init')\n",
      "('command executed successfuly:::', 'sudo apt-get install -y crudini')\n",
      "('command executed successfuly:::', 'crudini --set ~/airflow/airflow.cfg core load_examples False')\n",
      "('command executed successfuly:::', 'crudini --set ~/airflow/airflow.cfg core load_default_connections True')\n",
      "('command executed successfuly:::', 'crudini --set ~/airflow/airflow.cfg scheduler min_file_process_interval 10')\n",
      "('command executed successfuly:::', 'crudini --set ~/airflow/airflow.cfg scheduler dag_dir_list_interval 3')\n",
      "('command executed successfuly:::', 'airflow db init')\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/aws/aws-sam-cli/issues/3661\n",
    "run_via_ssh(ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'sudo pip3 install markupsafe==2.0.1',\n",
    "        'airflow db init',\n",
    "        'sudo apt-get install -y crudini',\n",
    "        \"crudini --set ~/airflow/airflow.cfg core load_examples False\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg core load_default_connections True\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg core sql_alchemy_conn 'mysql://airflow:airflow@localhost/airflow'\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg core executor CeleryExecutor\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg core sql_alchemy_schema airflow\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg scheduler min_file_process_interval 10\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg scheduler dag_dir_list_interval 3\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg celery result_backend 'redis://127.0.0.1:6379/0'\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg celery broker_url 'db+mysql://airflow:airflow@localhost/airflow'\",\n",
    "        'airflow db init',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Airflow dag directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('command executed successfuly:::', 'mkdir -p ~/airflow/dags')\n",
      "('command executed successfuly:::', 'mkdir -p ~/pipe/ml/data')\n",
      "('command executed successfuly:::', 'mkdir -p ~/pipe/ml/model')\n",
      "('command executed successfuly:::', 'mkdir -p ~/pipe/ml/working')\n",
      "('command executed successfuly:::', 'mkdir -p ~/pipe/ml/images')\n"
     ]
    }
   ],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'mkdir -p ~/airflow/dags',\n",
    "        f\"mkdir -p ~/pipe/ml/data\",\n",
    "        f\"mkdir -p ~/pipe/ml/model\",\n",
    "        f\"mkdir -p ~/pipe/ml/working\",\n",
    "        f\"mkdir -p ~/pipe/ml/images\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('command executed successfuly:::', 'airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin')\n",
      "('command executed successfuly:::', 'airflow scheduler -D')\n",
      "('command executed successfuly:::', 'airflow webserver -p 8080 -D')\n"
     ]
    }
   ],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin',\n",
    "        'airflow scheduler -D',\n",
    "#         'airflow celery worker -D',\n",
    "#         'airflow celery flower -D',\n",
    "        'airflow webserver -p 8080 -D'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accesing the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chmod 600 my-experimental-kp-june-2021-01.pem\n",
      "SSH      : ssh -i my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "WebServer: http://3.134.147.166:8080\n",
      "Login admin admin\n"
     ]
    }
   ],
   "source": [
    "print(f\"chmod 600 {ec2_pem_name}.pem\")\n",
    "print(f\"SSH      : ssh -i {ec2_pem_name}.pem ubuntu@{ec2_ip['PublicIp']}\")\n",
    "print(f\"WebServer: http://{ec2_ip['PublicIp']}:8080\")\n",
    "print(f\"Login admin admin\")\n",
    "# print(f\"Flower   : http://{ec2_ip['PublicIp']}:5555\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refshift Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a role for Redshift and attach the needed policies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift role my-experimental-Redshift-role-01 has been created. The policies were also attached. \n",
      "The Redshift role has been ceated with the ARN: arn:aws:iam::986106953013:role/my-experimental-Redshift-role-01\n"
     ]
    }
   ],
   "source": [
    "redshift_role = does_role_already_exist(my_redshift_role_name)\n",
    "\n",
    "if redshift_role is None:\n",
    "    redshift_role = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=my_redshift_role_name,\n",
    "        Description='role used for for capstone project',\n",
    "        MaxSessionDuration=3600,\n",
    "        AssumeRolePolicyDocument=\"\"\"{\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"redshift.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "      ]\n",
    "    }\"\"\")['Role']\n",
    "#     attaching the policies\n",
    "    for redshift_policy in  [\n",
    "        'arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "        'arn:aws:iam::aws:policy/AWSGlueConsoleFullAccess']:\n",
    "        assert iam.attach_role_policy(\n",
    "            RoleName=redshift_role['RoleName'],\n",
    "            PolicyArn=redshift_policy)['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    print(f'Redshift role {my_redshift_role_name} has been created. The policies were also attached. ')\n",
    "else:\n",
    "    print(f'Redshift role {my_redshift_role_name} already exists ')\n",
    "print(f\"The Redshift role has been ceated with the ARN: {redshift_role['Arn']}\")\n",
    "redshift_role_arn = redshift_role['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a security group for Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refshift security group sg-072159ad6a7faeb6c created successfuly\n"
     ]
    }
   ],
   "source": [
    "redshift_sg = ec2.create_security_group(\n",
    "    Description='Allows 5432 trafic',\n",
    "    GroupName='Redshift')\n",
    "ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=5439, ToPort=5439, GroupId=redshift_sg['GroupId'], IpProtocol='TCP')\n",
    "print(f\"Refshift security group {redshift_sg['GroupId']} created successfuly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate a public IP for Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift PublicIp: 52.14.252.20 for AllocationId: eipalloc-0017984f034e05430\n"
     ]
    }
   ],
   "source": [
    "redshift_ip = ec2.allocate_address(Domain='vpc')\n",
    "# [ redshift_ip['PublicIp'], redshift_ip['AllocationId'] ]\n",
    "print(f\"Redshift PublicIp: {redshift_ip['PublicIp']} for AllocationId: {redshift_ip['AllocationId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift cluster my-experimental-redshift-cluster has been created\n"
     ]
    }
   ],
   "source": [
    "# redshift_host = redshift_ip['PublicIp']\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "redshift_cluster = redshift.create_cluster(\n",
    "    DBName=redshift_db,\n",
    "    ClusterIdentifier=redshift_ClusterIdentifier,\n",
    "    NodeType=redshift_NodeType,\n",
    "    ClusterType=redshift_ClusterType,\n",
    "#     AvailabilityZone=my_region,\n",
    "#     change the next one (uncomment) if the redshift_ClusterType is multinode\n",
    "#     see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "#     NumberOfNodes=redshift_NumberOfNodes,\n",
    "    MasterUsername=redshift_user,\n",
    "    MasterUserPassword=redshift_MasterUserPassword,\n",
    "    VpcSecurityGroupIds=[ redshift_sg['GroupId'] ],\n",
    "    IamRoles=[ redshift_role['Arn'] ],\n",
    "    ElasticIp=redshift_ip['PublicIp'],\n",
    "    PubliclyAccessible=True,\n",
    "    Encrypted=False)['Cluster']\n",
    "redshift.get_waiter('cluster_available').wait(ClusterIdentifier=redshift_cluster['ClusterIdentifier'])\n",
    "print(f\"Redshift cluster {redshift_cluster['ClusterIdentifier']} has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: redshift\n",
      "password: kljhdfsKLJDD12345\n",
      "server: 52.14.252.20:5439/my-experimental-capstone-db\n",
      "Redshift connection string = 'postgresql://redshift:kljhdfsKLJDD12345@52.14.252.20:5439/my-experimental-capstone-db'\n"
     ]
    }
   ],
   "source": [
    "redshift_url = f\"postgresql://{redshift_user}:{redshift_MasterUserPassword}@{redshift_ip['PublicIp']}:5439/{redshift_db}\"\n",
    "print(f\"user: {redshift_user}\\npassword: {redshift_MasterUserPassword}\\nserver: {redshift_ip['PublicIp']}:5439/{redshift_db}\")    \n",
    "print(f\"Redshift connection string = '{redshift_url}'\")\n",
    "%sql $redshift_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the AWS covid-19 Data Lake      \n",
    "Abouy the covid-19 data lake : https://aws.amazon.com/blogs/big-data/a-public-data-lake-for-analysis-of-covid-19-data/     \n",
    "The url to the CloudFormation template that will create the data lake within the account :https://us-east-2.console.aws.amazon.com/cloudformation/home?region=us-east-2#/stacks/create/review?templateURL=https://covid19-lake.s3.us-east-2.amazonaws.com/cfn/CovidLakeStack.template.json&stackName=CovidLakeStack     \n",
    "Additional resources: https://aws.amazon.com/blogs/big-data/exploring-the-public-aws-covid-19-data-lake/     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow additional setup based on Redshift configuration parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airflow variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('command executed successfuly:::', \"airflow variables set 'model_training_data_path' '/home/ubuntu/pipe/ml/data/training.zip'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'raw_data_file' '/home/ubuntu/pipe/ml/data/raw_data_file.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'raw_data_file_empty_columns_eliminated' '/home/ubuntu/pipe/ml/data/raw_data_file_empty_columns_eliminated.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'eliminated_papers_older_than_01_01_2020' '/home/ubuntu/pipe/ml/data/eliminated_papers_older_than_01_01_2020.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'eliminated_non_english_languages' '/home/ubuntu/pipe/ml/data/eliminated_non_english_languages.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'spacy_preprocessed' '/home/ubuntu/pipe/ml/data/spacy_preprocessed.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'intermediate_preprocessed_s3_key' 'data/preprocessed/2022/2/25/intermediate_preprocessed.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'spacy_preprocessed_s3_key' 'data/preprocessed/2022/2/25/spacy_preprocessed.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_kmeans_number_of_clusters' '6'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_cord19_small_subset' '/home/ubuntu/pipe/ml/data/ml_cord19_small_subset.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_data_with_kmeans_applied' '/home/ubuntu/pipe/ml/data/ml_data_with_kmeans_applied.csv'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_simple_tsne_visualization_output' '/home/ubuntu/pipe/ml/images/ml_simple_tsne_visualization_output.png'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_improved_tsne_visualization_output' '/home/ubuntu/pipe/ml/images/ml_improved_tsne_visualization_output.png'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_number_of_topics_per_cluster' '5'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_model_doc2vec_output' '/home/ubuntu/pipe/ml/doc2vec.model'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_model_doc2vec_max_epoch' '300'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_model_doc2vec_vec_size' '100'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_model_doc2vec_alpha' '0.025'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'ml_topics_output' '/home/ubuntu/pipe/ml/data/ml_topics_output.txt'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'model_output_path' '/home/ubuntu/pipe/ml/model'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'unload_raw_data_to_s3_key' 'data/raw/2022/2/25'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'unload_raw_data_to_s3_filename' 'alleninstitute_metadata_000'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'model_max_length' '30'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'model_vocab_size' '10000'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'model_emb_dims' '64'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'model_lstm_units' '128'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'model_training_batch_size' '50'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'model_training_epochs' '5'\")\n",
      "('command executed successfuly:::', \"airflow variables set 's3_redshift_iam_role' 'arn:aws:iam::986106953013:role/my-experimental-Redshift-role-01'\")\n",
      "('command executed successfuly:::', \"airflow variables set 's3_redshift_region' 'us-east-2'\")\n",
      "('command executed successfuly:::', \"airflow variables set 's3_staging_bucket' 'dantohe-my-experimental-iac-02'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'redshift_db' 'my-experimental-capstone-db'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'redshift_destination_external_schema_name' 'spectrum_schema'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'redshift_destination_glue_database_name' 'covid-19'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'redshift_destination_table_name' 'alleninstitute_metadata'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'redshift_role_arn' 'arn:aws:iam::986106953013:role/my-experimental-Redshift-role-01'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'my_region' 'us-east-2'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'path_images_dir' '/home/ubuntu/pipe/ml/images'\")\n",
      "('command executed successfuly:::', \"airflow variables set 'path_working_dir' '/home/ubuntu/pipe/ml/working'\")\n",
      "('command executed successfuly:::', \"airflow variables set 's3_report_bucket' 'dantohe-my-experimental-iac-02'\")\n",
      "('command executed successfuly:::', \"airflow variables set 's3_report_folder' 'reports'\")\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime.today()\n",
    "date_partition  = f\"{dt.year}/{dt.month}/{dt.day}\"\n",
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        f\"airflow variables set 'model_training_data_path' '/home/ubuntu/pipe/ml/data/training.zip'\",\n",
    "        f\"airflow variables set 'raw_data_file' '/home/ubuntu/pipe/ml/data/raw_data_file.csv'\",\n",
    "        f\"airflow variables set 'raw_data_file_empty_columns_eliminated' '/home/ubuntu/pipe/ml/data/raw_data_file_empty_columns_eliminated.csv'\",\n",
    "        f\"airflow variables set 'eliminated_papers_older_than_01_01_2020' '/home/ubuntu/pipe/ml/data/eliminated_papers_older_than_01_01_2020.csv'\",\n",
    "        f\"airflow variables set 'eliminated_non_english_languages' '/home/ubuntu/pipe/ml/data/eliminated_non_english_languages.csv'\",\n",
    "        f\"airflow variables set 'spacy_preprocessed' '/home/ubuntu/pipe/ml/data/spacy_preprocessed.csv'\",\n",
    "        f\"airflow variables set 'intermediate_preprocessed_s3_key' 'data/preprocessed/{date_partition}/intermediate_preprocessed.csv'\",\n",
    "        f\"airflow variables set 'spacy_preprocessed_s3_key' 'data/preprocessed/{date_partition}/spacy_preprocessed.csv'\",\n",
    "        f\"airflow variables set 'ml_kmeans_number_of_clusters' '6'\",\n",
    "        f\"airflow variables set 'ml_cord19_small_subset' '/home/ubuntu/pipe/ml/data/ml_cord19_small_subset.csv'\",\n",
    "        f\"airflow variables set 'ml_data_with_kmeans_applied' '/home/ubuntu/pipe/ml/data/ml_data_with_kmeans_applied.csv'\",\n",
    "        f\"airflow variables set 'ml_simple_tsne_visualization_output' '/home/ubuntu/pipe/ml/images/ml_simple_tsne_visualization_output.png'\",\n",
    "        f\"airflow variables set 'ml_improved_tsne_visualization_output' '/home/ubuntu/pipe/ml/images/ml_improved_tsne_visualization_output.png'\",\n",
    "        f\"airflow variables set 'ml_number_of_topics_per_cluster' '5'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_output' '/home/ubuntu/pipe/ml/doc2vec.model'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_max_epoch' '300'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_vec_size' '100'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_alpha' '0.025'\",\n",
    "        f\"airflow variables set 'ml_topics_output' '/home/ubuntu/pipe/ml/data/ml_topics_output.txt'\",\n",
    "        f\"airflow variables set 'model_output_path' '/home/ubuntu/pipe/ml/model'\",\n",
    "        f\"airflow variables set 'unload_raw_data_to_s3_key' 'data/raw/{date_partition}'\",\n",
    "        f\"airflow variables set 'unload_raw_data_to_s3_filename' 'alleninstitute_metadata_000'\",\n",
    "        f\"airflow variables set 'model_max_length' '30'\",\n",
    "        f\"airflow variables set 'model_vocab_size' '10000'\",\n",
    "        f\"airflow variables set 'model_emb_dims' '64'\",\n",
    "        f\"airflow variables set 'model_lstm_units' '128'\",\n",
    "        f\"airflow variables set 'model_training_batch_size' '50'\",\n",
    "        f\"airflow variables set 'model_training_epochs' '5'\",\n",
    "        f\"airflow variables set 's3_redshift_iam_role' '{redshift_role['Arn']}'\",\n",
    "        f\"airflow variables set 's3_redshift_region' '{my_region}'\",\n",
    "        f\"airflow variables set 's3_staging_bucket' '{my_bucket_name}'\",\n",
    "        f\"airflow variables set 'redshift_db' '{redshift_db}'\",\n",
    "        f\"airflow variables set 'redshift_destination_external_schema_name' '{redshift_destination_external_schema_name}'\",\n",
    "        f\"airflow variables set 'redshift_destination_glue_database_name' '{redshift_destination_glue_database_name}'\",\n",
    "        f\"airflow variables set 'redshift_destination_table_name' '{redshift_destination_table_name}'\",\n",
    "        f\"airflow variables set 'redshift_role_arn' '{redshift_role_arn}'\",\n",
    "        f\"airflow variables set 'my_region' '{my_region}'\",\n",
    "        f\"airflow variables set 'path_images_dir' '/home/ubuntu/pipe/ml/images'\",\n",
    "        f\"airflow variables set 'path_working_dir' '/home/ubuntu/pipe/ml/working'\",\n",
    "        f\"airflow variables set 's3_report_bucket' '{my_bucket_name}'\",\n",
    "        f\"airflow variables set 's3_report_folder' 'reports'\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airflow Redshift connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'postgresql://redshift:kljhdfsKLJDD12345@52.14.252.20:5439/my-experimental-capstone-db'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('command executed successfuly:::', \" airflow connections add         'redshift_db' --conn-uri 'postgresql://redshift:kljhdfsKLJDD12345@52.14.252.20:5439/my-experimental-capstone-db'\")\n"
     ]
    }
   ],
   "source": [
    "redshift_endpoint = redshift_ip[\"PublicIp\"]\n",
    "\n",
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        f\"\"\" airflow connections add \\\n",
    "        'redshift_db' --conn-uri '{redshift_url}'\"\"\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dags into Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/spacy_utils.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/language_utils.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/transfer_utils.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/cord19_ml_dag.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/cleanup_utils.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/cord19_processing_dag.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/ml_utils_vectorization.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n",
      "ssh -i ./my-experimental-kp-june-2021-01.pem ubuntu@3.134.147.166\n",
      "scp -i my-experimental-kp-june-2021-01.pem 'dags/cord19_ml_small_subset_dag.py' 'ubuntu@3.134.147.166:~/airflow/dags'\n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(os.listdir('dags/')):\n",
    "    if filename.endswith(\".py\"):\n",
    "        upload_dag_file(\n",
    "            ip=ec2_ip['PublicIp'],\n",
    "            pem_path=ec2_pem_path,\n",
    "            file_name=f'dags/{filename}',\n",
    "            display_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift cluster deleted.\n"
     ]
    }
   ],
   "source": [
    "redshift.delete_cluster(ClusterIdentifier=redshift_cluster['ClusterIdentifier'], SkipFinalClusterSnapshot=True)\n",
    "redshift.get_waiter('cluster_deleted').wait(ClusterIdentifier=redshift_cluster['ClusterIdentifier'])\n",
    "print('Redshift cluster deleted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Redshift public IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '20bfc893-28cf-49c7-889a-ea63243dffbc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '20bfc893-28cf-49c7-889a-ea63243dffbc',\n",
       "   'cache-control': 'no-cache, no-store',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '229',\n",
       "   'date': 'Sat, 26 Feb 2022 15:12:22 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec2.release_address(AllocationId=redshift_ip['AllocationId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Redshift security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '93e7028f-8e48-4f62-97cd-a2374060c5f2',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '93e7028f-8e48-4f62-97cd-a2374060c5f2',\n",
       "   'cache-control': 'no-cache, no-store',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '239',\n",
       "   'date': 'Sat, 26 Feb 2022 15:12:25 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec2.delete_security_group(GroupId=redshift_sg['GroupId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Redshift role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '7fa0d7d8-c651-4d5e-801d-5fb0014c20af',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7fa0d7d8-c651-4d5e-801d-5fb0014c20af',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '200',\n",
       "   'date': 'Sat, 26 Feb 2022 15:12:30 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for attached_policy in iam.list_attached_role_policies(RoleName=redshift_role['RoleName'])['AttachedPolicies']:\n",
    "        iam.detach_role_policy(RoleName=redshift_role['RoleName'], PolicyArn=attached_policy['PolicyArn'])\n",
    "for policy_name in iam.list_role_policies(RoleName=redshift_role['RoleName'])['PolicyNames']:\n",
    "    iam.delete_role_policy(RoleName=redshift_role['RoleName'], PolicyName=policy_name)\n",
    "iam.delete_role(RoleName=redshift_role['RoleName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel the spot instance request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CancelledSpotInstanceRequests': [{'SpotInstanceRequestId': 'sir-zgaya5xq',\n",
       "   'State': 'cancelled'}],\n",
       " 'ResponseMetadata': {'RequestId': '127ca97c-b834-4523-a569-3eb89b36dbfb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '127ca97c-b834-4523-a569-3eb89b36dbfb',\n",
       "   'cache-control': 'no-cache, no-store',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '426',\n",
       "   'date': 'Sat, 26 Feb 2022 23:39:19 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec2.cancel_spot_instance_requests(SpotInstanceRequestIds=[ ec2_spot_id ])\n",
    "# ec2.cancel_spot_instance_requests(SpotInstanceRequestIds=[ 'sir-rk8sj4bj' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the spot instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TerminatingInstances': [{'CurrentState': {'Code': 32,\n",
       "    'Name': 'shutting-down'},\n",
       "   'InstanceId': 'i-0941cc99318e0a3a0',\n",
       "   'PreviousState': {'Code': 16, 'Name': 'running'}}],\n",
       " 'ResponseMetadata': {'RequestId': '4ae56049-f9a8-45be-af4c-d21708d9230c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4ae56049-f9a8-45be-af4c-d21708d9230c',\n",
       "   'cache-control': 'no-cache, no-store',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'accept-encoding',\n",
       "   'content-type': 'text/xml;charset=UTF-8',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'date': 'Sat, 26 Feb 2022 23:39:23 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec2.terminate_instances(InstanceIds=[ ec2_vm_id ])\n",
    "# ec2.terminate_instances(InstanceIds=[ 'i-080cf3bee321c8357' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the instance to terminate and release the IP address "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '0f405339-12f0-4b24-9d7d-4f2cba116c30',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0f405339-12f0-4b24-9d7d-4f2cba116c30',\n",
       "   'cache-control': 'no-cache, no-store',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '229',\n",
       "   'date': 'Sat, 26 Feb 2022 23:40:57 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec2.get_waiter('instance_terminated').wait(InstanceIds=[ ec2_vm_id ])\n",
    "ec2.release_address(AllocationId=ec2_ip['AllocationId'])\n",
    "\n",
    "#if stuck needs to be removed manualy from the console ( VPC -> Elastic IPs)\n",
    "# ec2.get_waiter('instance_terminated').wait(InstanceIds=['i-080cf3bee321c8357'])\n",
    "# ec2.release_address(AllocationId='eipassoc-026af42e9e3b0fb02')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.delete_security_group(GroupId=ec2_sg['GroupId'])\n",
    "\n",
    "# ec2.delete_security_group(GroupId='sg-0a1592d4d67e19433')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the key-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.delete_key_pair(KeyName=ec2_pem_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detach and delete the role policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attached_policy in iam.list_attached_role_policies(RoleName=ec2_role['RoleName'])['AttachedPolicies']:\n",
    "    iam.detach_role_policy(RoleName=ec2_role['RoleName'], PolicyArn=attached_policy['PolicyArn'])\n",
    "for policy_name in iam.list_role_policies(RoleName=ec2_role['RoleName'])['PolicyNames']:\n",
    "    iam.delete_role_policy(RoleName=ec2_role['RoleName'], PolicyName=policy_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove role from instance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.remove_role_from_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'], RoleName=ec2_role['RoleName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete instance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam.delete_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.delete_role(RoleName=ec2_role['RoleName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences    \n",
    "https://www.cloudwalker.io/2019/09/30/airflow-scale-out-with-redis-and-celery/   \n",
    "https://aws.amazon.com/blogs/big-data/a-public-data-lake-for-analysis-of-covid-19-data/     \n",
    "https://us-east-2.console.aws.amazon.com/cloudformation/home?region=us-east-2#/stacks/create/review?templateURL=https://covid19-lake.s3.us-east-2.amazonaws.com/cfn/CovidLakeStack.template.json&stackName=CovidLakeStack\n",
    "https://aws.amazon.com/blogs/big-data/exploring-the-public-aws-covid-19-data-lake/   \n",
    "https://medium.com/@hudsonmendes/data-pipeline-for-data-science-part-1-problem-solution-fit-3b092880efa3     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
