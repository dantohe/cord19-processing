{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b50e818",
   "metadata": {},
   "source": [
    "# AWS Resources deployment using Infrastructure-as-Code (on a Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bd8f8",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c129c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m pip install --upgrade pip\n",
    "# !python3 -m pip install boto3\n",
    "# !python3 -m pip install requests\n",
    "# !python3 -m pip install tqdm\n",
    "# !python3 -m pip install pandas\n",
    "# !python3 -m pip install s3fs\n",
    "# !python3 -m pip install ipywidgets\n",
    "# !python3 -m pip install -q -U paramiko\n",
    "# !python3 -m pip install -q -U scp\n",
    "# !pip install -q -U ipython-sql\n",
    "# !pip install -q -U psycopg2-binary\n",
    "# !python3 -m pip install -q -U Pygments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54dfed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import boto3\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import paramiko\n",
    "import scp\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50989864",
   "metadata": {},
   "source": [
    "#### Some basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62cfb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#core\n",
    "my_bucket_name = 'dantohe-my-experimental-iac-02'\n",
    "# the covid-19 data lake is located in us-east-2\n",
    "my_region = 'us-east-2'\n",
    "stem = 'my-experimental'\n",
    "\n",
    "#ec2\n",
    "my_InstanceProfileName = f'{stem}-InstanceProfileName-iac-01'\n",
    "ec2_pem_name      = f'{stem}-kp-june-2021-01'\n",
    "my_role_name = f'{stem}-ec2-role-01'\n",
    "my_security_group_name = f'{stem}-Airflow-security-group-01'\n",
    "# https://aws.amazon.com/ec2/spot/pricing/\n",
    "# t3.2xlarge\n",
    "instance_size = 't3.xlarge'\n",
    "instance_max_price = '0.12'\n",
    "\n",
    "#redshift\n",
    "my_redshift_role_name = f'{stem}-Redshift-role-01'\n",
    "redshift_port = 5439\n",
    "redshift_user = 'redshift'\n",
    "# Only printable ASCII characters except for '/', '@', '\"', ' ', '\\', ''' may be used.\n",
    "redshift_MasterUserPassword = 'kljhdfsKLJDD12345'\n",
    "redshift_db=f'{stem}-capstone-db'\n",
    "redshift_ClusterIdentifier=f'{stem}-redshift-cluster'\n",
    "#https://aws.amazon.com/redshift/pricing/\n",
    "redshift_NodeType='dc2.large'\n",
    "# https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\n",
    "redshift_NumberOfNodes=1\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "redshift_ClusterType='single-node'\n",
    "\n",
    "redshift_destination_external_schema_name = 'spectrum_schema'\n",
    "redshift_destination_glue_database_name = 'covid-19'\n",
    "redshift_destination_table_name = 'alleninstitute_metadata'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541672f",
   "metadata": {},
   "source": [
    "#### Configuring the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1aca579",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = Config(\n",
    "    region_name = my_region,\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "# client = boto3.client('kinesis', config=my_config)\n",
    "ec2 = boto3.client('ec2', config=my_config)\n",
    "iam = boto3.client('iam', config=my_config)\n",
    "redshift = boto3.client('redshift', config=my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0355898",
   "metadata": {},
   "source": [
    "#### Create an EC2 key-pair    \n",
    "If the key already exists then don't do anything.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741e4b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key already exists\n"
     ]
    }
   ],
   "source": [
    "key_exists = False\n",
    "response = ec2.describe_key_pairs()['KeyPairs']\n",
    "for key in response:\n",
    "    if key['KeyName'] == ec2_pem_name:\n",
    "        key_exists = True\n",
    "    found_instance = ec2.describe_instances(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'key-name',\n",
    "                'Values': [key['KeyName']]\n",
    "            }\n",
    "        ]\n",
    "    )['Reservations']\n",
    "\n",
    "if key_exists:\n",
    "    print('key already exists')\n",
    "else:\n",
    "    ec2_pem_path = f'./{ec2_pem_name}.pem'\n",
    "    if os.path.isfile(ec2_pem_path):\n",
    "        os.remove(ec2_pem_path)\n",
    "    ec2_keypair = ec2.create_key_pair(KeyName=ec2_pem_name)\n",
    "    with open(ec2_pem_path, 'w+') as ec2_pem_file:\n",
    "        ec2_pem_file.write(str(ec2_keypair['KeyMaterial']))\n",
    "    !chmod 400 {ec2_pem_path}\n",
    "    print(f'{ec2_pem_name} has been created sucessfully and the pem is available at\\n{ec2_pem_path}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615c413",
   "metadata": {},
   "source": [
    "## EC2 Resources    \n",
    "IAM - refrences: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#role   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5071b",
   "metadata": {},
   "source": [
    "### Create a role and attach the s3 access policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44797423",
   "metadata": {},
   "source": [
    "#### A utility that checks if a given role already exists    \n",
    "If already in place returns the role object, otherwise returns a None.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4749c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_role_already_exist(role_name):\n",
    "    roles = iam.list_roles()\n",
    "    role_list = roles['Roles']\n",
    "    requested_role= None\n",
    "\n",
    "    for role in role_list:\n",
    "        if role['RoleName'] == role_name:\n",
    "            requested_role = role\n",
    "            return requested_role\n",
    "    return requested_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958014d",
   "metadata": {},
   "source": [
    "### Create role for ec2 and attach s3 access policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39198df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role my-experimental-ec2-role-01 already exists\n"
     ]
    }
   ],
   "source": [
    "roles = iam.list_roles()\n",
    "role_list = roles['Roles']\n",
    "ec2_role= None\n",
    "\n",
    "for key in role_list:\n",
    "    if key['RoleName'] == my_role_name:\n",
    "        ec2_role = key\n",
    "\n",
    "if ec2_role is not None:\n",
    "    print(f'Role {my_role_name} already exists')\n",
    "else:\n",
    "    ec2_role = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=my_role_name,\n",
    "        Description='',\n",
    "        MaxSessionDuration=3600,\n",
    "        AssumeRolePolicyDocument=\"\"\"{\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": { \"Service\": \"ec2.amazonaws.com\"},\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "      ]\n",
    "    }\"\"\".replace('<dw_bucket>', my_bucket_name))['Role']\n",
    "    ### Also atach the S3 policy to the role\n",
    "    for ec2_policy in  [\n",
    "        'arn:aws:iam::aws:policy/AmazonS3FullAccess']:\n",
    "        assert iam.attach_role_policy(\n",
    "            RoleName=ec2_role['RoleName'],\n",
    "            PolicyArn=ec2_policy)['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    \n",
    "    print(f'{my_role_name} has been createed - the S3 policies have been attached')\n",
    "#     ec2_role['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62d2cf",
   "metadata": {},
   "source": [
    "### Creates the instance profile AND adds the role to instance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2d5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-experimental-InstanceProfileName-iac-01 already exists\n"
     ]
    }
   ],
   "source": [
    "instance_profiles = iam.list_instance_profiles()\n",
    "instance_profiles_list = instance_profiles['InstanceProfiles']\n",
    "ec2_instance_profile = None\n",
    "\n",
    "# existing_instance_profile_names =[]\n",
    "\n",
    "for key in instance_profiles_list:\n",
    "    if key['InstanceProfileName'] == my_InstanceProfileName:\n",
    "        ec2_instance_profile =key\n",
    "#     existing_instance_profile_names.append(key['InstanceProfileName'])\n",
    "\n",
    "# if my_InstanceProfileName in existing_instance_profile_names:\n",
    "#     print(f'{my_InstanceProfileName} already exists')\n",
    "if ec2_instance_profile is not None:\n",
    "    print(f'{my_InstanceProfileName} already exists')\n",
    "else:\n",
    "    #creates the instaance profile\n",
    "    ec2_instance_profile = iam.create_instance_profile(InstanceProfileName=my_InstanceProfileName)['InstanceProfile']\n",
    "    iam.get_waiter('instance_profile_exists').wait(InstanceProfileName=my_InstanceProfileName)\n",
    "\n",
    "    #adds the role to the instance profile\n",
    "    assert iam.add_role_to_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'], RoleName=ec2_role['RoleName'])['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    print(f'{my_InstanceProfileName} has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b6411",
   "metadata": {},
   "source": [
    "### Createting a security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8a7fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The security group my-experimental-Airflow-security-group-01 already exists\n"
     ]
    }
   ],
   "source": [
    "security_groups = ec2.describe_security_groups()\n",
    "existing_security_groups = security_groups['SecurityGroups']\n",
    "\n",
    "ec2_sg = None\n",
    "\n",
    "for key in existing_security_groups:\n",
    "    if key['GroupName'] == my_security_group_name:\n",
    "      ec2_sg=key  \n",
    "    \n",
    "if ec2_sg is not None:\n",
    "    print(f'The security group {my_security_group_name} already exists')\n",
    "else:\n",
    "    ec2_sg = ec2.create_security_group(\n",
    "        Description='Allows 22 trafic',\n",
    "        GroupName=my_security_group_name)\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=22, ToPort=22, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=8080, ToPort=8080, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=5555, ToPort=5555, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=3306, ToPort=3306, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "    print(f'The security group {my_security_group_name} has been created')\n",
    "    print(f\"SG ID: {ec2_sg['GroupId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6102897",
   "metadata": {},
   "source": [
    "### Requesting spot instance(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08217c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in Region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "# !aws configure set region 'us-east-2'\n",
    "my_session = boto3.session.Session()\n",
    "my_region = my_session.region_name\n",
    "ags_west = boto3.client('autoscaling', region_name=my_region)\n",
    "print(f\"We are in Region: {my_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01595111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(300) #wait instance profile...\n",
    "#Amazon Linux AMI - it has some issues and complications with installing mysql and airflow\n",
    "# ec2_ami_id = 'ami-0aeeebd8d2ab47354'\n",
    "#defaulting to ubuntu\n",
    "# us-east-1 'ami-09e67e426f25ce0d7'\n",
    "# us-west-2 'ami-03d5c68bab01f3496'\n",
    "# us-east-2 'ami-00399ec92321828f5'\n",
    "ec2_ami_id = 'ami-00399ec92321828f5'\n",
    "ec2_spot = ec2.request_spot_instances(\n",
    "    AvailabilityZoneGroup=my_region,\n",
    "    InstanceCount=1,\n",
    "    LaunchSpecification={\n",
    "        'SecurityGroupIds': [ec2_sg['GroupId']],\n",
    "        'EbsOptimized': False,\n",
    "        'KeyName': ec2_pem_name,\n",
    "        'ImageId': ec2_ami_id,\n",
    "        'InstanceType': instance_size,\n",
    "        'IamInstanceProfile': {\n",
    "            'Arn': ec2_instance_profile['Arn']\n",
    "        },\n",
    "        \"BlockDeviceMappings\": [\n",
    "            {\n",
    "                \"DeviceName\": \"/dev/sda1\",\n",
    "                \"Ebs\": {\n",
    "                        \"DeleteOnTermination\": True,\n",
    "                        \"VolumeSize\": 30,\n",
    "                        \"Encrypted\": False,\n",
    "                        \"VolumeType\": \"gp2\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    SpotPrice=instance_max_price,\n",
    "    Type='one-time',\n",
    "    InstanceInterruptionBehavior='terminate'\n",
    ")\n",
    "ec2_spot_id = ec2_spot['SpotInstanceRequests'][0]['SpotInstanceRequestId']\n",
    "ec2.get_waiter('spot_instance_request_fulfilled').wait(SpotInstanceRequestIds=[ec2_spot_id])\n",
    "print(f'Spot instance request: {ec2_spot_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f456a",
   "metadata": {},
   "source": [
    "### Gets the instance ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a77e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_vm_id = ec2.describe_spot_instance_requests(SpotInstanceRequestIds=[ ec2_spot_id ]) \\\n",
    "    ['SpotInstanceRequests'] \\\n",
    "    [0] \\\n",
    "    ['InstanceId']\n",
    "ec2.get_waiter('instance_status_ok').wait(InstanceIds=[ ec2_vm_id ])\n",
    "print(f'InstanceIds: {ec2_vm_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a38fb",
   "metadata": {},
   "source": [
    "### Allocating a public IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_ip = ec2.allocate_address(Domain='vpc')\n",
    "print(f\"PublicIp: {ec2_ip['PublicIp']}\\nAllocationId: {ec2_ip['AllocationId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1afbd6",
   "metadata": {},
   "source": [
    "### Associates the IP address with the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b128aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_vm_ip = ec2.associate_address(\n",
    "     InstanceId = ec2_vm_id,\n",
    "     AllocationId = ec2_ip[\"AllocationId\"])\n",
    "print(f\"IP AssociationId: {ec2_vm_ip['AssociationId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455f016",
   "metadata": {},
   "source": [
    "## SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0de3b",
   "metadata": {},
   "source": [
    "### SSH utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ssh(ip, pem_path):\n",
    "    print(f\"ssh -i {pem_path} ubuntu@{ip}\")\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(hostname=ip, username='ubuntu', pkey=paramiko.RSAKey.from_private_key_file(pem_path))\n",
    "    return ssh\n",
    "\n",
    "def run_via_ssh(\n",
    "        ip,\n",
    "        pem_path,\n",
    "        commands,\n",
    "        display_output=False):\n",
    "    \n",
    "    ssh = get_ssh(ip, pem_path)\n",
    "    try:\n",
    "        for command in tqdm(commands):\n",
    "            stdin, stdout, stderr = ssh.exec_command(command)\n",
    "            exit_status = stdout.channel.recv_exit_status()\n",
    "            if exit_status == 0:\n",
    "                print(('command executed successfuly:::', command))\n",
    "                if display_output:\n",
    "                    output_buffer = stdout.read().decode('utf-8')\n",
    "                    if output_buffer:\n",
    "                        print(f\">>> {output_buffer}\")\n",
    "            else:\n",
    "                error_buffer = stderr.read().decode('utf-8')\n",
    "                print(('!!!failed', command))\n",
    "                print(f\"!!! {error_buffer}\")\n",
    "    finally:\n",
    "        ssh.close()\n",
    "\n",
    "# \n",
    "def print_python_code(code):\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import PythonLexer\n",
    "    from pygments.formatters import HtmlFormatter\n",
    "    import IPython\n",
    "    formatter = HtmlFormatter()\n",
    "    return IPython.display.HTML('<style type=\"text/css\">{}</style>{}'.format(\n",
    "        formatter.get_style_defs('.highlight'),\n",
    "        highlight(code, PythonLexer(), formatter)))\n",
    "   \n",
    "    \n",
    "def upload_dag_file(ip, pem_path, file_name, family_dir='airflow', display_file=True):\n",
    "    code = None\n",
    "    file_path = f'{file_name}'\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    if display_file:\n",
    "        with open(file_path) as f:\n",
    "            code = f.read()\n",
    "        if not code:\n",
    "            return None\n",
    "    \n",
    "    ssh = get_ssh(ip, pem_path)\n",
    "    try:\n",
    "        remote_file_dir = f'~/{family_dir}/{file_dir}'\n",
    "        print(f\"scp -i my-experimental-kp-june-2021-01.pem '{file_path}' 'ubuntu@{ip}:{remote_file_dir}'\")\n",
    "        scp_client = scp.SCPClient(ssh.get_transport())\n",
    "        scp_client.put(files=[file_path], remote_path=remote_file_dir, preserve_times=True)\n",
    "    finally:\n",
    "        ssh.close()\n",
    "    \n",
    "    if display_file:\n",
    "        return print_python_code(code)\n",
    "    else:\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b65a1",
   "metadata": {},
   "source": [
    "### Instaling Airflow and libraries\n",
    "This can be achieved using a requirements file but for better visibility it will be kept in this format for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c074cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'sudo apt-get -y update',\n",
    "#         'sudo apt-get install -y libmysqlclient-dev mysql-server',\n",
    "#         f\"sudo mysql -e \\\"SET GLOBAL explicit_defaults_for_timestamp = 1;\\\"\",\n",
    "#         f\"sudo mysql -e \\\"DROP DATABASE IF EXISTS airflow;\\\"\",       \n",
    "#         f\"sudo mysql -e \\\"CREATE DATABASE airflow CHARACTER SET UTF8mb3 COLLATE utf8_general_ci;\\\"\",\n",
    "#         f\"sudo mysql -e \\\"CREATE USER 'airflow'@'localhost' IDENTIFIED BY 'airflow';\\\"\",\n",
    "#         f\"sudo mysql -e \\\"GRANT ALL PRIVILEGES ON airflow.* TO 'airflow'@'localhost';\\\"\",\n",
    "#         f\"sudo apt install -y redis-server\",\n",
    "        'sudo apt-get install -y python3 python3-pip python3-setuptools',\n",
    "        'sudo pip3 install -U pip',\n",
    "        'sudo pip3 install -U apache-airflow',\n",
    "#         'sudo pip3 install -U apache-airflow[mysql]',\n",
    "#         'sudo pip3 install -U apache-airflow[celery]',\n",
    "        'sudo pip3 install -U boto3==1.15.0',\n",
    "        'sudo pip3 install -U jsonpath_ng==1.5.3',\n",
    "        'sudo pip3 install -U pandas==1.4.0',\n",
    "        'sudo pip3 install -U redshift_connector==2.0.888',\n",
    "        'sudo pip3 install -U sqlalchemy_redshift==0.8.6',\n",
    "        'sudo pip3 install -U watchtower==2.0.1',\n",
    "        'sudo pip3 install -U apache-airflow-providers-amazon',\n",
    "        'sudo pip3 install -U tensorflow',\n",
    "        'sudo pip3 install -U pandas',\n",
    "        'sudo pip3 install -U scikit-learn',\n",
    "        'sudo pip3 install -U numpy',\n",
    "        'sudo pip3 install -U psycopg2-binary',\n",
    "        'sudo pip3 install -U requests',\n",
    "        'sudo pip3 install -U boto3',\n",
    "        'sudo pip3 install -U matplotlib',\n",
    "        'sudo pip3 install -U reportlab',\n",
    "        'sudo pip3 install -U flower',\n",
    "        'sudo pip3 install -U proj',\n",
    "        'sudo pip3 install -U apache-airflow[postgres]',\n",
    "        'sudo pip3 install -U apache-airflow[amazon]',\n",
    "        'sudo pip3 install -U tqdm',\n",
    "        'sudo pip3 install -U langdetect',\n",
    "        'sudo pip3 install -U seaborn',\n",
    "        'sudo pip3 install -U gensim',\n",
    "        'sudo pip3 install -U nltk',\n",
    "        'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz',\n",
    "        'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz',\n",
    "        'sudo pip3 install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz',\n",
    "        'sudo pip3 install -U redis'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db733fa",
   "metadata": {},
   "source": [
    "### Configure Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aws/aws-sam-cli/issues/3661\n",
    "run_via_ssh(ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'sudo pip3 install markupsafe==2.0.1',\n",
    "        'airflow db init',\n",
    "        'sudo apt-get install -y crudini',\n",
    "        \"crudini --set ~/airflow/airflow.cfg core load_examples False\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg core load_default_connections True\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg core sql_alchemy_conn 'mysql://airflow:airflow@localhost/airflow'\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg core executor CeleryExecutor\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg core sql_alchemy_schema airflow\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg scheduler min_file_process_interval 10\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg scheduler dag_dir_list_interval 3\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg celery result_backend 'redis://127.0.0.1:6379/0'\",\n",
    "#         \"crudini --set ~/airflow/airflow.cfg celery broker_url 'db+mysql://airflow:airflow@localhost/airflow'\",\n",
    "        'airflow db init',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053e85e",
   "metadata": {},
   "source": [
    "### Create Airflow dag directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'mkdir -p ~/airflow/dags',\n",
    "        f\"mkdir -p ~/pipe/ml/data\",\n",
    "        f\"mkdir -p ~/pipe/ml/model\",\n",
    "        f\"mkdir -p ~/pipe/ml/working\",\n",
    "        f\"mkdir -p ~/pipe/ml/images\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45c878",
   "metadata": {},
   "source": [
    "### Start Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72690a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin',\n",
    "        'airflow scheduler -D',\n",
    "#         'airflow celery worker -D',\n",
    "#         'airflow celery flower -D',\n",
    "        'airflow webserver -p 8080 -D'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f76cc4",
   "metadata": {},
   "source": [
    "### Accesing the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"chmod 600 {ec2_pem_name}.pem\")\n",
    "print(f\"SSH      : ssh -i {ec2_pem_name}.pem ubuntu@{ec2_ip['PublicIp']}\")\n",
    "print(f\"WebServer: http://{ec2_ip['PublicIp']}:8080\")\n",
    "print(f\"Login admin admin\")\n",
    "# print(f\"Flower   : http://{ec2_ip['PublicIp']}:5555\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d8887",
   "metadata": {},
   "source": [
    "## Refshift Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0081f80",
   "metadata": {},
   "source": [
    "### Creates a role for Redshift and attach the needed policies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_role = does_role_already_exist(my_redshift_role_name)\n",
    "\n",
    "if redshift_role is None:\n",
    "    redshift_role = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=my_redshift_role_name,\n",
    "        Description='role used for for capstone project',\n",
    "        MaxSessionDuration=3600,\n",
    "        AssumeRolePolicyDocument=\"\"\"{\n",
    "      \"Version\": \"2012-10-17\",\n",
    "      \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"redshift.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "      ]\n",
    "    }\"\"\")['Role']\n",
    "#     attaching the policies\n",
    "    for redshift_policy in  [\n",
    "        'arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "        'arn:aws:iam::aws:policy/AWSGlueConsoleFullAccess']:\n",
    "        assert iam.attach_role_policy(\n",
    "            RoleName=redshift_role['RoleName'],\n",
    "            PolicyArn=redshift_policy)['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    print(f'Redshift role {my_redshift_role_name} has been created. The policies were also attached. ')\n",
    "else:\n",
    "    print(f'Redshift role {my_redshift_role_name} already exists ')\n",
    "print(f\"The Redshift role has been ceated with the ARN: {redshift_role['Arn']}\")\n",
    "redshift_role_arn = redshift_role['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fe2ac",
   "metadata": {},
   "source": [
    "### Create a security group for Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a78f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_sg = ec2.create_security_group(\n",
    "    Description='Allows 5432 trafic',\n",
    "    GroupName='Redshift')\n",
    "ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=5439, ToPort=5439, GroupId=redshift_sg['GroupId'], IpProtocol='TCP')\n",
    "print(f\"Refshift security group {redshift_sg['GroupId']} created successfuly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757d40b",
   "metadata": {},
   "source": [
    "### Allocate a public IP for Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df14e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_ip = ec2.allocate_address(Domain='vpc')\n",
    "# [ redshift_ip['PublicIp'], redshift_ip['AllocationId'] ]\n",
    "print(f\"Redshift PublicIp: {redshift_ip['PublicIp']} for AllocationId: {redshift_ip['AllocationId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6c843",
   "metadata": {},
   "source": [
    "### Create a Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fff5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redshift_host = redshift_ip['PublicIp']\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "redshift_cluster = redshift.create_cluster(\n",
    "    DBName=redshift_db,\n",
    "    ClusterIdentifier=redshift_ClusterIdentifier,\n",
    "    NodeType=redshift_NodeType,\n",
    "    ClusterType=redshift_ClusterType,\n",
    "#     AvailabilityZone=my_region,\n",
    "#     change the next one (uncomment) if the redshift_ClusterType is multinode\n",
    "#     see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "#     NumberOfNodes=redshift_NumberOfNodes,\n",
    "    MasterUsername=redshift_user,\n",
    "    MasterUserPassword=redshift_MasterUserPassword,\n",
    "    VpcSecurityGroupIds=[ redshift_sg['GroupId'] ],\n",
    "    IamRoles=[ redshift_role['Arn'] ],\n",
    "    ElasticIp=redshift_ip['PublicIp'],\n",
    "    PubliclyAccessible=True,\n",
    "    Encrypted=False)['Cluster']\n",
    "redshift.get_waiter('cluster_available').wait(ClusterIdentifier=redshift_cluster['ClusterIdentifier'])\n",
    "print(f\"Redshift cluster {redshift_cluster['ClusterIdentifier']} has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aaad87",
   "metadata": {},
   "source": [
    "### Connecting to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f53741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9262b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_url = f\"postgresql://{redshift_user}:{redshift_MasterUserPassword}@{redshift_ip['PublicIp']}:5439/{redshift_db}\"\n",
    "print(f\"user: {redshift_user}\\npassword: {redshift_MasterUserPassword}\\nserver: {redshift_ip['PublicIp']}:5439/{redshift_db}\")    \n",
    "print(f\"Redshift connection string = '{redshift_url}'\")\n",
    "%sql $redshift_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683d3dd",
   "metadata": {},
   "source": [
    "### Using the AWS covid-19 Data Lake      \n",
    "Abouy the covid-19 data lake : https://aws.amazon.com/blogs/big-data/a-public-data-lake-for-analysis-of-covid-19-data/     \n",
    "The url to the CloudFormation template that will create the data lake within the account :https://us-east-2.console.aws.amazon.com/cloudformation/home?region=us-east-2#/stacks/create/review?templateURL=https://covid19-lake.s3.us-east-2.amazonaws.com/cfn/CovidLakeStack.template.json&stackName=CovidLakeStack     \n",
    "Additional resources: https://aws.amazon.com/blogs/big-data/exploring-the-public-aws-covid-19-data-lake/     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c3002",
   "metadata": {},
   "source": [
    "### Airflow additional setup based on Redshift configuration parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366fae5b",
   "metadata": {},
   "source": [
    "#### Airflow variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c33aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime.today()\n",
    "date_partition  = f\"{dt.year}/{dt.month}/{dt.day}\"\n",
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        f\"airflow variables set 'model_training_data_path' '/home/ubuntu/pipe/ml/data/training.zip'\",\n",
    "        f\"airflow variables set 'raw_data_file' '/home/ubuntu/pipe/ml/data/raw_data_file.csv'\",\n",
    "        f\"airflow variables set 'raw_data_file_empty_columns_eliminated' '/home/ubuntu/pipe/ml/data/raw_data_file_empty_columns_eliminated.csv'\",\n",
    "        f\"airflow variables set 'eliminated_papers_older_than_01_01_2020' '/home/ubuntu/pipe/ml/data/eliminated_papers_older_than_01_01_2020.csv'\",\n",
    "        f\"airflow variables set 'eliminated_non_english_languages' '/home/ubuntu/pipe/ml/data/eliminated_non_english_languages.csv'\",\n",
    "        f\"airflow variables set 'spacy_preprocessed' '/home/ubuntu/pipe/ml/data/spacy_preprocessed.csv'\",\n",
    "        f\"airflow variables set 'intermediate_preprocessed_s3_key' 'data/preprocessed/{date_partition}/intermediate_preprocessed.csv'\",\n",
    "        f\"airflow variables set 'spacy_preprocessed_s3_key' 'data/preprocessed/{date_partition}/spacy_preprocessed.csv'\",\n",
    "        f\"airflow variables set 'ml_kmeans_number_of_clusters' '6'\",\n",
    "        f\"airflow variables set 'ml_cord19_small_subset' '/home/ubuntu/pipe/ml/data/ml_cord19_small_subset.csv'\",\n",
    "        f\"airflow variables set 'ml_data_with_kmeans_applied' '/home/ubuntu/pipe/ml/data/ml_data_with_kmeans_applied.csv'\",\n",
    "        f\"airflow variables set 'ml_simple_tsne_visualization_output' '/home/ubuntu/pipe/ml/images/ml_simple_tsne_visualization_output.png'\",\n",
    "        f\"airflow variables set 'ml_improved_tsne_visualization_output' '/home/ubuntu/pipe/ml/images/ml_improved_tsne_visualization_output.png'\",\n",
    "        f\"airflow variables set 'ml_number_of_topics_per_cluster' '5'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_output' '/home/ubuntu/pipe/ml/doc2vec.model'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_max_epoch' '300'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_vec_size' '100'\",\n",
    "        f\"airflow variables set 'ml_model_doc2vec_alpha' '0.025'\",\n",
    "        f\"airflow variables set 'ml_topics_output' '/home/ubuntu/pipe/ml/data/ml_topics_output.txt'\",\n",
    "        f\"airflow variables set 'model_output_path' '/home/ubuntu/pipe/ml/model'\",\n",
    "        f\"airflow variables set 'unload_raw_data_to_s3_key' 'data/raw/{date_partition}'\",\n",
    "        f\"airflow variables set 'unload_raw_data_to_s3_filename' 'alleninstitute_metadata_000'\",\n",
    "        f\"airflow variables set 'model_max_length' '30'\",\n",
    "        f\"airflow variables set 'model_vocab_size' '10000'\",\n",
    "        f\"airflow variables set 'model_emb_dims' '64'\",\n",
    "        f\"airflow variables set 'model_lstm_units' '128'\",\n",
    "        f\"airflow variables set 'model_training_batch_size' '50'\",\n",
    "        f\"airflow variables set 'model_training_epochs' '5'\",\n",
    "        f\"airflow variables set 's3_redshift_iam_role' '{redshift_role['Arn']}'\",\n",
    "        f\"airflow variables set 's3_redshift_region' '{my_region}'\",\n",
    "        f\"airflow variables set 's3_staging_bucket' '{my_bucket_name}'\",\n",
    "        f\"airflow variables set 'redshift_db' '{redshift_db}'\",\n",
    "        f\"airflow variables set 'redshift_destination_external_schema_name' '{redshift_destination_external_schema_name}'\",\n",
    "        f\"airflow variables set 'redshift_destination_glue_database_name' '{redshift_destination_glue_database_name}'\",\n",
    "        f\"airflow variables set 'redshift_destination_table_name' '{redshift_destination_table_name}'\",\n",
    "        f\"airflow variables set 'redshift_role_arn' '{redshift_role_arn}'\",\n",
    "        f\"airflow variables set 'my_region' '{my_region}'\",\n",
    "        f\"airflow variables set 'path_images_dir' '/home/ubuntu/pipe/ml/images'\",\n",
    "        f\"airflow variables set 'path_working_dir' '/home/ubuntu/pipe/ml/working'\",\n",
    "        f\"airflow variables set 's3_report_bucket' '{my_bucket_name}'\",\n",
    "        f\"airflow variables set 's3_report_folder' 'reports'\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7c821",
   "metadata": {},
   "source": [
    "#### Airflow Redshift connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7228b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_endpoint = redshift_ip[\"PublicIp\"]\n",
    "\n",
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        f\"\"\" airflow connections add \\\n",
    "        'redshift_db' --conn-uri '{redshift_url}'\"\"\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d74b1",
   "metadata": {},
   "source": [
    "#### Loading the dags into Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir('dags/')):\n",
    "    if filename.endswith(\".py\"):\n",
    "        upload_dag_file(\n",
    "            ip=ec2_ip['PublicIp'],\n",
    "            pem_path=ec2_pem_path,\n",
    "            file_name=f'dags/{filename}',\n",
    "            display_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7a089",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10cc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d313726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a275d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765addf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f60b992",
   "metadata": {},
   "source": [
    "### Delete Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift.delete_cluster(ClusterIdentifier=redshift_cluster['ClusterIdentifier'], SkipFinalClusterSnapshot=True)\n",
    "redshift.get_waiter('cluster_deleted').wait(ClusterIdentifier=redshift_cluster['ClusterIdentifier'])\n",
    "print('Redshift cluster deleted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85586295",
   "metadata": {},
   "source": [
    "### Release Redshift public IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.release_address(AllocationId=redshift_ip['AllocationId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24969eca",
   "metadata": {},
   "source": [
    "### Delete Redshift security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.delete_security_group(GroupId=redshift_sg['GroupId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f48be",
   "metadata": {},
   "source": [
    "### Delete Redshift role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f98e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attached_policy in iam.list_attached_role_policies(RoleName=redshift_role['RoleName'])['AttachedPolicies']:\n",
    "        iam.detach_role_policy(RoleName=redshift_role['RoleName'], PolicyArn=attached_policy['PolicyArn'])\n",
    "for policy_name in iam.list_role_policies(RoleName=redshift_role['RoleName'])['PolicyNames']:\n",
    "    iam.delete_role_policy(RoleName=redshift_role['RoleName'], PolicyName=policy_name)\n",
    "iam.delete_role(RoleName=redshift_role['RoleName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074494a",
   "metadata": {},
   "source": [
    "### Cancel the spot instance request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810452db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.cancel_spot_instance_requests(SpotInstanceRequestIds=[ ec2_spot_id ])\n",
    "# ec2.cancel_spot_instance_requests(SpotInstanceRequestIds=[ 'sir-rk8sj4bj' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8e2fb",
   "metadata": {},
   "source": [
    "### Terminate the spot instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.terminate_instances(InstanceIds=[ ec2_vm_id ])\n",
    "# ec2.terminate_instances(InstanceIds=[ 'i-080cf3bee321c8357' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3560e",
   "metadata": {},
   "source": [
    "### Wait for the instance to terminate and release the IP address "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.get_waiter('instance_terminated').wait(InstanceIds=[ ec2_vm_id ])\n",
    "ec2.release_address(AllocationId=ec2_ip['AllocationId'])\n",
    "\n",
    "#if stuck needs to be removed manualy from the console ( VPC -> Elastic IPs)\n",
    "# ec2.get_waiter('instance_terminated').wait(InstanceIds=['i-080cf3bee321c8357'])\n",
    "# ec2.release_address(AllocationId='eipassoc-026af42e9e3b0fb02')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979251c",
   "metadata": {},
   "source": [
    "### Delete security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57524644",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.delete_security_group(GroupId=ec2_sg['GroupId'])\n",
    "\n",
    "# ec2.delete_security_group(GroupId='sg-0a1592d4d67e19433')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4cf06",
   "metadata": {},
   "source": [
    "### Delete the key-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.delete_key_pair(KeyName=ec2_pem_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996abe27",
   "metadata": {},
   "source": [
    "### Detach and delete the role policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8539bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attached_policy in iam.list_attached_role_policies(RoleName=ec2_role['RoleName'])['AttachedPolicies']:\n",
    "    iam.detach_role_policy(RoleName=ec2_role['RoleName'], PolicyArn=attached_policy['PolicyArn'])\n",
    "for policy_name in iam.list_role_policies(RoleName=ec2_role['RoleName'])['PolicyNames']:\n",
    "    iam.delete_role_policy(RoleName=ec2_role['RoleName'], PolicyName=policy_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efeed7",
   "metadata": {},
   "source": [
    "### Remove role from instance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.remove_role_from_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'], RoleName=ec2_role['RoleName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259e8ba",
   "metadata": {},
   "source": [
    "### Delete instance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9477a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam.delete_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198057fc",
   "metadata": {},
   "source": [
    "### Delete the role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16578c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.delete_role(RoleName=ec2_role['RoleName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1b78e",
   "metadata": {},
   "source": [
    "## ALL DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d639b",
   "metadata": {},
   "source": [
    "## Refrences    \n",
    "https://www.cloudwalker.io/2019/09/30/airflow-scale-out-with-redis-and-celery/   \n",
    "https://aws.amazon.com/blogs/big-data/a-public-data-lake-for-analysis-of-covid-19-data/     \n",
    "https://us-east-2.console.aws.amazon.com/cloudformation/home?region=us-east-2#/stacks/create/review?templateURL=https://covid19-lake.s3.us-east-2.amazonaws.com/cfn/CovidLakeStack.template.json&stackName=CovidLakeStack\n",
    "https://aws.amazon.com/blogs/big-data/exploring-the-public-aws-covid-19-data-lake/   \n",
    "https://medium.com/@hudsonmendes/data-pipeline-for-data-science-part-1-problem-solution-fit-3b092880efa3     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a06f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
